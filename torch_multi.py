import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


# root ë°ì´í„° ê²½ë¡œ
# train í•™ìŠµìš© í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ì—¬ë¶€
# download=True ì¸í„°ë„·ì—ì„œ ë‹¤ìš´
# transform ì´ë¯¸ì§€ ë³€í™˜ - > torchì—ì„œë„ ì“¸ ìˆ˜ ìˆê²Œ
training_data = datasets.FashionMNIST(
    root= "data",
    train= True,
    download=True,
    transform= ToTensor()

)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

# indexë¡œ ë°ì´í„° ì ‘ê·¼
labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}

# plt í¬ê¸°ì¡°ì ˆ
figure = plt.figure(figsize=(8,8))


# ë°ì´í„° ì²´í¬
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()



# DataLoader ì„¤ì •

batch_size = 32

train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)





# ëª¨ë¸ ì •ì˜
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()  # ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
        self.fc1 = nn.Linear(28*28, 128)  # ì²« ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ
        self.fc2 = nn.Linear(128, 64)  # ë‘ ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ
        self.fc3 = nn.Linear(64, 10)  # ì¶œë ¥ì¸µ (10ê°œ í´ë˜ìŠ¤)

    def forward(self, x):
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ëª¨ë¸ ìƒì„±
model = NeuralNetwork()


# ì†ì‹¤ í•¨ìˆ˜
loss_fn = nn.CrossEntropyLoss()

# ì˜µí‹°ë§ˆì´ì € (SGD ì‚¬ìš©)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# optimizer = optim.Adam(model.parameters(), lr=0.001)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)  # ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ê°€ëŠ¥í•˜ë©´)

epochs = 5  # í•™ìŠµ íšŸìˆ˜

for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")

    for batch, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)

        # ì˜ˆì¸¡
        outputs = model(images)
        loss = loss_fn(outputs, labels)

        # ì—­ì „íŒŒ
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            print(f"Batch {batch}, Loss: {loss.item():.4f}")

print("í•™ìŠµ ì™„ë£Œ!")


correct = 0
total = 0

model.eval()  # í‰ê°€ ëª¨ë“œ
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {100 * correct / total:.2f}%")



"""

âœ… CNN vs. nn.Linear (MLP)
CNN(í•©ì„±ê³± ì‹ ê²½ë§)
â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ì— íŠ¹í™”, íŒ¨í„´(ì—£ì§€, í…ìŠ¤ì²˜ ë“±)ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œ
â†’ Conv2d(í•©ì„±ê³±) + ReLU + MaxPool2d ì‚¬ìš©
â†’ ì˜ˆ: ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€

nn.Linear (MLP, ì™„ì „ì—°ê²° ì‹ ê²½ë§)
â†’ ëª¨ë“  ë‰´ëŸ°ì´ ì„œë¡œ ì—°ê²°, ì…ë ¥ì„ ë‹¨ìˆœí•œ ìˆ«ìë¡œ ë³€í™˜
â†’ Linear + ReLU ì‚¬ìš©
â†’ ì˜ˆ: ìˆ«ì ë°ì´í„°, NLP, ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ë¥˜

âœ… ì˜µí‹°ë§ˆì´ì € ì—­í• 
ì‹ ê²½ë§ì´ ë” ì¢‹ì€ ê°€ì¤‘ì¹˜(weight)ë¥¼ ì°¾ë„ë¡ ì—…ë°ì´íŠ¸
ì†ì‹¤(loss)ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
ì˜ˆ:
SGD (í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•): ê°„ë‹¨í•˜ì§€ë§Œ ëŠë¦¼
Adam: ë¹ ë¥´ê³  íš¨ìœ¨ì , ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨
âœ… ìˆœì „íŒŒ(Forward) vs. ì—­ì „íŒŒ(Backward)
ìˆœì „íŒŒ (Forward Pass)
â†’ ì…ë ¥ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ì„ í†µí•´ ì˜ˆì¸¡ê°’ ì¶œë ¥

ì—­ì „íŒŒ (Backward Pass)
â†’ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì • (ì˜¤ì°¨ ì „íŒŒ)
â†’ loss.backward()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸° ê³„ì‚°

ğŸ‘‰ ìˆœì „íŒŒ â†’ ê²°ê³¼ ì˜ˆì¸¡
ğŸ‘‰ ì—­ì „íŒŒ â†’ ì˜¤ì°¨ ìˆ˜ì • (í•™ìŠµ ì§„í–‰)

"""