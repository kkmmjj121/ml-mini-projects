import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
california = fetch_california_housing()
df = pd.DataFrame(california.data, columns=california.feature_names)
df["Target"] = california.target  # ì§‘ê°’(Target) ì¶”ê°€

# ë°ì´í„° í™•ì¸
print(df.head())  # ìƒìœ„ 5ê°œ ë°ì´í„° ì¶œë ¥
print(df.info())  # ë°ì´í„° íƒ€ì… í™•ì¸
print(df.describe())  # í†µê³„ ìš”ì•½

print(df.isnull().sum())

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
X = df.drop(columns=["Target"])  # ë…ë¦½ ë³€ìˆ˜ (íŠ¹ì§• ë°ì´í„°)
y = df["Target"]  # ì¢…ì† ë³€ìˆ˜ (ì§‘ê°’)

# í›ˆë ¨ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‚˜ëˆ„ê¸°
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë°ì´í„° ì •ê·œí™” (í‘œì¤€í™”)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ìˆ˜í–‰
y_pred = model.predict(X_test)

# ëª¨ë¸ í‰ê°€ (í‰ê· ì œê³±ì˜¤ì°¨, MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")

# ê°€ì¤‘ì¹˜(ê¸°ìš¸ê¸°)ì™€ ì ˆí¸ ì¶œë ¥
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

# ì‹¤ì œ ê°’ vs ì˜ˆì¸¡ ê°’ ë¹„êµ ê·¸ë˜í”„
plt.scatter(y_test, y_pred, alpha=0.5, color="blue")
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted Home Prices")
plt.show()


"""
1ï¸âƒ£ ê°€ì¤‘ì¹˜(ê¸°ìš¸ê¸°)ì™€ ì ˆí¸ì„ ì¶œë ¥í•˜ëŠ” ì´ìœ 
ê°€ì¤‘ì¹˜ (Weights, Coefficients): ê° ì…ë ¥ ë³€ìˆ˜(X)ê°€ ê²°ê³¼ê°’(y)ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„
ì ˆí¸ (Intercept): ì…ë ¥ê°’ì´ ëª¨ë‘ 0ì¼ ë•Œì˜ ì˜ˆì¸¡ê°’
ğŸ“Œ ì¶œë ¥í•˜ëŠ” ì´ìœ ?

ëª¨ë¸ì´ ì–´ë–¤ íŠ¹ì§•(ë³€ìˆ˜)ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
ì˜ˆë¥¼ ë“¤ì–´, ê°€ì¤‘ì¹˜ê°€ í¬ë‹¤ë©´ ê·¸ ë³€ìˆ˜ëŠ” ì§‘ê°’ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì˜ë¯¸!
2ï¸âƒ£ MSE ë§ê³ ë„ íšŒê·€ í‰ê°€ ì§€í‘œ

///////from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

âœ” MSE (Mean Squared Error, í‰ê· ì œê³±ì˜¤ì°¨)

ì˜¤ì°¨ì˜ ì œê³±ì„ í‰ê· ë‚¸ ê°’ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
ë‹¨ìœ„ê°€ í¼ â†’ í•´ì„ì´ ì–´ë ¤ìš¸ ìˆ˜ë„ ìˆìŒ
âœ” RMSE (Root Mean Squared Error, í‰ê· ì œê³±ê·¼ì˜¤ì°¨)

MSEì— ì œê³±ê·¼ì„ ì”Œìš´ ê°’
ì›ë˜ ê°’(y)ê³¼ ê°™ì€ ë‹¨ìœ„ë¥¼ ê°€ì§ â†’ í•´ì„ì´ ì‰¬ì›€
RMSE = np.sqrt(MSE)
âœ” MAE (Mean Absolute Error, í‰ê· ì ˆëŒ€ì˜¤ì°¨)

ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ì„ í‰ê· ë‚¸ ê²ƒ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
MSEë³´ë‹¤ ì´ìƒì¹˜(Outlier)ì— ëœ ë¯¼ê°
âœ” RÂ² Score (ê²°ì •ê³„ìˆ˜, ì„¤ëª…ë ¥)

0 ~ 1 ì‚¬ì´ ê°’ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„


âœ” RÂ² Score (ê²°ì •ê³„ìˆ˜) â†’ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„ (1ì´ë©´ ì™„ë²½í•œ ì˜ˆì¸¡)

âœ” MSE, RMSE, MAE â†’ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
ê°’ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ ì˜¤ì°¨ê°€ ì ë‹¤ëŠ” ì˜ë¯¸


3ï¸âƒ£ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ì 
âœ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Missing Values)

df.isnull().sum() ìœ¼ë¡œ í™•ì¸ í›„ ì±„ìš°ê±°ë‚˜ ì‚­ì œ
ìˆ«ìí˜• ë°ì´í„° â†’ í‰ê· , ì¤‘ì•™ê°’ ëŒ€ì²´ (df.fillna(df.mean()))
ë²”ì£¼í˜• ë°ì´í„° â†’ ìµœë¹ˆê°’ ëŒ€ì²´
âœ” ì •ê·œí™” (Normalization) / í‘œì¤€í™” (Standardization)

ë°ì´í„° í¬ê¸° ì°¨ì´ë¥¼ ë§ì¶° ëª¨ë¸ í•™ìŠµì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¦
í‘œì¤€í™”: StandardScaler() (í‰ê·  0, ë¶„ì‚° 1)
ì •ê·œí™”: MinMaxScaler() (0~1 ì‚¬ì´ë¡œ ë³€í™˜)
âœ” í›ˆë ¨ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬

ë°ì´í„° ê³¼ì í•© ë°©ì§€ (train_test_split() ì‚¬ìš©)
ì¼ë°˜ì ìœ¼ë¡œ 80:20 ë¹„ìœ¨ ì‚¬ìš©
"""