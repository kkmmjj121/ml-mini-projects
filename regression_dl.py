from sklearn.datasets import fetch_openml
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim


# Boston Housing ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
data = fetch_openml(name="boston", version=1, as_frame=True)
df = data.frame

# ë°ì´í„° í™•ì¸
print(df.head())


# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())

# ê²°ì¸¡ì¹˜ ì œê±°
df = df.dropna()


X = df.drop(columns=['MEDV'])  # 'MEDV'ê°€ ì§‘ê°’(íƒ€ê²Ÿ)
y = df['MEDV']


scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)  # 0~1 ë²”ìœ„ë¡œ ë³€í™˜


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)


# Tensor ë³€í™˜
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

"""
1ï¸âƒ£ numpyë‚˜ pandas ë°ì´í„°ë¥¼ PyTorch Tensorë¡œ ë³€í™˜
â†’ torch.tensor(X_train, dtype=torch.float32)
â†’ dtype=torch.float32ëŠ” PyTorchê°€ ì—°ì‚°ì„ ì˜ ìˆ˜í–‰í•˜ë„ë¡ ì‹¤ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

2ï¸âƒ£ íƒ€ê²Ÿ ë°ì´í„°(y_train, y_test) ë³€í™˜ ì‹œ .values ì‚¬ìš©
â†’ pandas.Seriesë¥¼ Numpy ë°°ì—´ë¡œ ë³€í™˜í•´ì„œ torch.tensor()ì— ë„£ìŒ

3ï¸âƒ£ view(-1, 1) â†’ ì°¨ì› ë³€ê²½ (reshape)

(-1, 1)ì€ 1ì—´ì§œë¦¬ 2D í…ì„œë¡œ ë³€í™˜
ì˜ˆë¥¼ ë“¤ì–´ y_trainì´ [5, 10, 15] ê°™ì€ í˜•íƒœë¼ë©´
view(-1, 1)ì„ ì ìš©í•˜ë©´ [[5], [10], [15]]ë¡œ ë³€í™˜
ğŸ“Œ ì´ ê³¼ì •ì´ í•„ìš”í•œ ì´ìœ 
âœ… PyTorch ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì…ë ¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§ì¶”ê¸° ìœ„í•´
âœ… íŠ¹íˆ y ê°’ì€ ë‹¨ì¼ ê°’ì´ë¼ë„ 2D í˜•íƒœ ([batch_size, 1])ë¡œ ë§ì¶°ì•¼ í•¨

âœ” í•œ ì¤„ ìš”ì•½
ğŸ‘‰ pandas/numpy ë°ì´í„°ë¥¼ PyTorch Tensorë¡œ ë³€í™˜í•˜ê³ ,
ğŸ‘‰ yê°’ì„ 2Dë¡œ reshapeí•´ì„œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ê³¼ì •! ğŸš€
"""



# ëª¨ë¸ ì •ì˜
class RegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(RegressionModel, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.layer(x)

"""
âœ” nn.Moduleì„ ìƒì†í•˜ì—¬ ëª¨ë¸ ì •ì˜
âœ” __init__()ì—ì„œ ë ˆì´ì–´ ì •ì˜ (nn.Linear, nn.ReLU)
âœ” forward()ì—ì„œ ì…ë ¥ ë°ì´í„°ê°€ ì–´ë–»ê²Œ íë¥´ëŠ”ì§€ ì§€ì •
âœ” nn.Sequential()ë¡œ ê°„ë‹¨í•˜ê²Œ ë ˆì´ì–´ êµ¬ì„± ê°€ëŠ¥

ğŸ“Œ ê²°ë¡ :
nn.Linear() â†’ ì„ í˜• ë³€í™˜ (Wx + b)
nn.ReLU() â†’ í™œì„±í™” í•¨ìˆ˜ ì ìš© (ë¹„ì„ í˜•ì„± ì¶”ê°€)
âœ” ì´ë ‡ê²Œ ì¸µì„ ìŒ“ì•„ í•™ìŠµ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“ ë‹¤! ğŸš€

"""


# ëª¨ë¸ ìƒì„±
model = RegressionModel(input_dim=X_train.shape[1])

# ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)


# í•™ìŠµ
epochs = 1000
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()

    y_pred = model(X_train_tensor)
    loss = loss_fn(y_pred, y_train_tensor)

    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# í‰ê°€
model.eval()
with torch.no_grad():
    y_pred_test = model(X_test_tensor)
    test_loss = loss_fn(y_pred_test, y_test_tensor)
    print(f"Test Loss: {test_loss.item():.4f}")
